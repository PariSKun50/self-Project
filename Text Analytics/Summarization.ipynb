{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a Simple Summarizer in Python from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/write-a-simple-summarizer-in-python-e9ca6138a08e\n",
    "\n",
    "Step 1. Read from source — Read the unabridged content from the source, a file in the case of this exercise.\n",
    "\n",
    "Step 2. Perform formatting and cleanup — Format and clean up our format so that it is free of extra white space or other issues.\n",
    "\n",
    "Step 3. Tokenize input — Take the input and break it up into its individual words.\n",
    "\n",
    "Step 4. Scoring — Score (count) the frequency of each word in the input and score sentences based on word score.\n",
    "\n",
    "Step 5. Selection — Choose the top N sentences based on their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            return file.read()\n",
    "    except IOError as e:\n",
    "        print(\"Fatal Error: File ({}) could not be located or is not readable.\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_input(data):\n",
    "    replace = {\n",
    "        ord('\\f') : ' ',\n",
    "        ord('\\t') : ' ',\n",
    "        ord('\\n') : ' ',\n",
    "        ord('\\r') : None\n",
    "    }\n",
    "    return data.translate(replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_content(content):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = word_tokenize(content.lower())\n",
    "    \n",
    "    return [\n",
    "        sent_tokenize(content),\n",
    "        [word for word in words if word not in stop_words]    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_tokens(filtered_words, sentence_tokens):\n",
    "    word_freq = FreqDist(filtered_words)\n",
    "    ranking = defaultdict(int)\n",
    "    for i, sentence in enumerate(sentence_tokens):\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_freq:\n",
    "                ranking[i] += word_freq[word]\n",
    "    return ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(ranks, sentences, length):\n",
    "    if int(length) > len(sentences): \n",
    "        print(\"Error, more sentences requested than available. Use --l (--length) flag to adjust.\")\n",
    "        exit()\n",
    "    indexes = nlargest(length, ranks, key=ranks.get)\n",
    "    final_sentences = [sentences[j] for j in sorted(indexes)]\n",
    "    return ' '.join(final_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet.  But the real action at East GRIP is under the surface. Researchers are drilling through more than 2.5 kilometers of ice, down to the bedrock below. The ice is sliding fast - for a glacier - toward the sea. Scientists here want to know why. The answer may hold clues to the future of the world's coastal cities.  Greenland is melting. As it melts, it adds roughly 1 millimeter of water per year to global sea levels. And the pace of melting is quickening.  If all the ice covering the world's largest island were to thaw, sea levels would rise roughly 6 meters. Scientists don't know how fast, or how likely, that is to happen. East GRIP is looking for evidence to inform both those questions.  The answers are a matter of growing urgency. The seas are rising faster. And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further.  The Arctic is warming twice as fast as the rest of the planet. Scientists studying the rapid changes gather in the small Greenland town of Kangerlussuaq, a former U.S. military base built during World War II. Through the Cold War, this outpost supplied remote radar sites watching for a nuclear attack coming over the pole.  These days, military transport planes fly scientists and their equipment across 1,000 kilometers of Arctic ice to East GRIP. They make research possible here and at other far-flung scientific outposts on the vast Greenland ice sheet.  Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change. \n",
      "\n",
      "[\"Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet.\", 'But the real action at East GRIP is under the surface.', 'Researchers are drilling through more than 2.5 kilometers of ice, down to the bedrock below.', 'The ice is sliding fast - for a glacier - toward the sea.', 'Scientists here want to know why.', \"The answer may hold clues to the future of the world's coastal cities.\", 'Greenland is melting.', 'As it melts, it adds roughly 1 millimeter of water per year to global sea levels.', 'And the pace of melting is quickening.', \"If all the ice covering the world's largest island were to thaw, sea levels would rise roughly 6 meters.\", \"Scientists don't know how fast, or how likely, that is to happen.\", 'East GRIP is looking for evidence to inform both those questions.', 'The answers are a matter of growing urgency.', 'The seas are rising faster.', \"And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further.\", 'The Arctic is warming twice as fast as the rest of the planet.', 'Scientists studying the rapid changes gather in the small Greenland town of Kangerlussuaq, a former U.S. military base built during World War II.', 'Through the Cold War, this outpost supplied remote radar sites watching for a nuclear attack coming over the pole.', 'These days, military transport planes fly scientists and their equipment across 1,000 kilometers of Arctic ice to East GRIP.', 'They make research possible here and at other far-flung scientific outposts on the vast Greenland ice sheet.', 'Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change.'] \n",
      "\n",
      "['like', 'bowling', 'ball', 'skating', 'rink', 'black', 'geodesic', 'sphere', 'east', 'greenland', 'ice-core', 'project', \"'s\", 'communal', 'living', 'space', 'stands', 'endless', 'white', 'nothingness', 'greenland', 'ice', 'sheet', 'real', 'action', 'east', 'grip', 'surface', 'researchers', 'drilling', '2.5', 'kilometers', 'ice', 'bedrock', 'ice', 'sliding', 'fast', 'glacier', 'toward', 'sea', 'scientists', 'want', 'know', 'answer', 'may', 'hold', 'clues', 'future', 'world', \"'s\", 'coastal', 'cities', 'greenland', 'melting', 'melts', 'adds', 'roughly', '1', 'millimeter', 'water', 'per', 'year', 'global', 'sea', 'levels', 'pace', 'melting', 'quickening', 'ice', 'covering', 'world', \"'s\", 'largest', 'island', 'thaw', 'sea', 'levels', 'would', 'rise', 'roughly', '6', 'meters', 'scientists', \"n't\", 'know', 'fast', 'likely', 'happen', 'east', 'grip', 'looking', 'evidence', 'inform', 'questions', 'answers', 'matter', 'growing', 'urgency', 'seas', 'rising', 'faster', 'processes', 'work', 'greenland', \"'s\", 'glaciers', 'top', 'world', 'could', 'send', 'vast', 'sections', 'antarctica', \"'s\", 'ice', 'sheet', 'sea', 'well', 'raising', 'ocean', 'levels', 'even', 'arctic', 'warming', 'twice', 'fast', 'rest', 'planet', 'scientists', 'studying', 'rapid', 'changes', 'gather', 'small', 'greenland', 'town', 'kangerlussuaq', 'former', 'u.s.', 'military', 'base', 'built', 'world', 'war', 'ii', 'cold', 'war', 'outpost', 'supplied', 'remote', 'radar', 'sites', 'watching', 'nuclear', 'attack', 'coming', 'pole', 'days', 'military', 'transport', 'planes', 'fly', 'scientists', 'equipment', 'across', '1,000', 'kilometers', 'arctic', 'ice', 'east', 'grip', 'make', 'research', 'possible', 'far-flung', 'scientific', 'outposts', 'vast', 'greenland', 'ice', 'sheet', 'departing', 'kangerlussuaq', 'voa', 'visited', 'east', 'grip', 'remote', 'corners', 'greenland', '109th', 'airlift', 'wing', 'u.s.', 'air', 'national', 'guard', 'firsthand', 'look', 'science', 'action', 'leading', 'edge', 'climate', 'change'] \n",
      "\n",
      "defaultdict(<class 'int'>, {0: 51, 1: 13, 2: 13, 3: 17, 4: 7, 5: 16, 6: 9, 7: 17, 8: 4, 9: 33, 10: 12, 11: 13, 12: 4, 13: 3, 14: 52, 15: 9, 16: 33, 17: 14, 18: 33, 19: 25, 20: 41}) \n",
      "\n",
      "Like a bowling ball on a skating rink, the black geodesic sphere of the East Greenland Ice-Core Project's communal living space stands out against the endless white nothingness of the Greenland ice sheet. And the same processes at work on Greenland's glaciers at the top of the world could send vast sections of Antarctica's ice sheet into the sea as well, raising ocean levels even further. Departing from Kangerlussuaq, VOA visited East GRIP and other remote corners of Greenland with the 109th Airlift Wing of the U.S. Air National Guard for a firsthand look at science in action at the leading edge of climate change.\n"
     ]
    }
   ],
   "source": [
    "content = read_file(\"Greenland-Melting-Full.txt\")\n",
    "content = sanitize_input(content)\n",
    "print(content,\"\\n\")\n",
    "sentence_tokens, word_tokens = tokenize_content(content)  \n",
    "print(sentence_tokens,\"\\n\")\n",
    "print(word_tokens,\"\\n\")\n",
    "    \n",
    "sentence_ranks = score_tokens(word_tokens, sentence_tokens)\n",
    "print(sentence_ranks,\"\\n\")\n",
    "\n",
    "print(summarize(sentence_ranks, sentence_tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://ai.intelligentonlinetools.com/ml/text-summarization/\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    " \n",
    "def get_only_text(url):\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    "    return soup.title.text, text    \n",
    " \n",
    "url=\"https://en.wikipedia.org/wiki/Deep_learning\"\n",
    "text = get_only_text(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization using sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--LsaSummarizer--\n",
      "[138] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\\n Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks.\n",
      "[142] Deep neural architectures provide the best results for constituency parsing,[143] sentiment analysis,[144] information retrieval,[145][146] spoken language understanding,[147] machine translation,[103][148] contextual entity linking,[148] writing style recognition,[149] Text classification and others.\n",
      "[216] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[220] and artificial intelligence (AI).\n",
      "\n",
      "--LuhnSummarizer--\n",
      "[2]\\n Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks,  convolutional neural networks and Transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n",
      "For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\\n Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\n",
      "Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\\n Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\\n As of 2017, neural networks typically have a few thousand to a few million units and millions of connections.\n",
      "\n",
      "--EdmundsonSummarizer--\n",
      "('Deep learning - Wikipedia', 'Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n",
      "For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\\n Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\n",
      "[12] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    " \n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.edmundson import EdmundsonSummarizer   #found this is the best as \n",
    "# it is picking from beginning also while other skip\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 3\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    url=\"https://en.wikipedia.org/wiki/Deep_learning\"\n",
    "    text = get_only_text(url)\n",
    "    parser = PlaintextParser(text, Tokenizer(LANGUAGE))\n",
    "    # or for plain text files\n",
    "    # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    " \n",
    "    print (\"--LsaSummarizer--\")\n",
    "    summarizer = LsaSummarizer(Stemmer(LANGUAGE))#Latent Semantic Analysis\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "         \n",
    "    print (\"\\n--LuhnSummarizer--\")     \n",
    "    summarizer = LuhnSummarizer(Stemmer(LANGUAGE)) #Word freq method\n",
    "    summarizer.stop_words = (\"I\", \"am\", \"the\", \"you\", \"are\", \"me\", \"is\", \"than\", \"that\", \"this\",)\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)\n",
    "         \n",
    "    print (\"\\n--EdmundsonSummarizer--\")     \n",
    "    summarizer = EdmundsonSummarizer() #cue phrase method\n",
    "    words = (\"deep\", \"learning\", \"neural\" )\n",
    "    summarizer.bonus_words = words\n",
    "    words = (\"another\", \"and\", \"some\", \"next\",)\n",
    "    summarizer.stigma_words = words\n",
    "    words = (\"another\", \"and\", \"some\", \"next\",)\n",
    "    summarizer.null_words = words\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LexRank using sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, upon learning that the gang now has in their possession both Hyo-jeong and So-mi, Tae-sik gives the beaten gang members what they are looking for.\n",
      "Now with the knowledge that So-mee is being used by an ANT organization to secretly smuggle drugs and - in the future might be killed for organ harvesting, Tae-sik goes on a mad killing.hunting spree to locate and save So-mee, who is his only connection to a caring world.\n",
      "Tae-sik went mad when a container that holds harvested eyes was rolled towards him, believing to be So-mees.\n",
      "A police escort in the end sees Tae-sik and So-mee together in the back of the detectives car.\n",
      "Tae-sik pulls up a fancy backpack with a star, and fills it up with fancy school stuff, much to her delight.\n"
     ]
    }
   ],
   "source": [
    "text='''Operating a pawn shop in a small neighborhood, Cha Tae-sik now leads a quiet life. His only connection to the rest of the world is a little girl, So-mee, who lives nearby. A heroin addict and So-mis mother, Hyo-jeong, smuggles drugs from a drug trafficking organization and entrusts Tae-sik with the product without his knowledge. When the traffickers find out about this they kidnap both Hyo-jeong and So-mi. The gang sends a number of thugs to Tae-sik's pawn shop to retrieve the stolen drugs, but is easily overpowered by Tae-sik, making his identity ambiguous. However, upon learning that the gang now has in their possession both Hyo-jeong and So-mi, Tae-sik gives the beaten gang members what they are looking for.\n",
    "Realizing that Tae-sik may serve better as a mule than their former thug, the brothers that lead the gang Man-sik and Jong-sik promise to release Hyo-jeong and So-mi under the condition that Tae-sik make a delivery for them. Tae-sik makes the decision to face the outside world in order to rescue So-mi. However, the delivery was part of a larger plot to eliminate a drug ring superior, Mr. Oh, and Tae-sik is arrested. At the same time, Hyo-jeongs body, with her organs harvested, is discovered in the back of the car used by Tae-sik when he made the delivery, and Tae-sik realizes that So-mis life may also be in danger. He fights off half a dozen detectives and escapes from the police station. During his escape, the police are bewildered at Tae-sik's display of power, combat techniques and agility, and further investigates his bio and finds out that he was once a black operation agent for the Korean government with numerous commendations, but dropped out from the agency after witnessing his pregnant wife being murdered in front of his eyes in connection with him by being hit by a truck while inside their car. The incident nearly drove him mad, hence he went into hiding.\n",
    "Realizing this, the Narcotics head contacted a weakened Tae-sik after his encounter with a highly trained assassin who backs up the brothers. Now with the knowledge that So-mee is being used by an ANT organization to secretly smuggle drugs and - in the future might be killed for organ harvesting, Tae-sik goes on a mad killing.hunting spree to locate and save So-mee, who is his only connection to a caring world.\n",
    "A gore battle ensues, from the ANT/Drug manufacturing location where he was able to free the remaining children and kill off the younger of the brothers, to their posh condo unit where the rest of the killing continues. Tae-sik went mad when a container that holds harvested eyes was rolled towards him, believing to be So-mees. A final stand-off with the assassin ensues, with Tae-sik winning the fight.\n",
    "After killing off the last hoodlum in the parking lot, Tae-sik was about to resign to his fate by shooting himself when a scared and dirty So-mee emerged from the darkness. She was saved by the assassin who took to her kindly and killed instead the surgeon accomplice - his eyes were in the container that Tae-sik saw.\n",
    "A police escort in the end sees Tae-sik and So-mee together in the back of the detectives car. While she sleeps. Tae-sik asks if they can be dropped off at the small convenience store - a surprise for So-mee. The owner got a shock upon seeing all the police lined up around them, and with the words: You really messed it up big time.\n",
    "Tae-sik pulls up a fancy backpack with a star, and fills it up with fancy school stuff, much to her delight.\n",
    "Tae-sik asks if he can hug her, and as they embrace, he could not stop the tears as they fell from his battered bandaged face.'''\n",
    "\n",
    "import sumy\n",
    "from sumy.parsers.plaintext import PlaintextParser #We're choosing a plaintext parser here, other parsers available for HTML etc.\n",
    "from sumy.nlp.tokenizers import Tokenizer \n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer #We're choosing Lexrank, other algorithms are also built in\n",
    "\n",
    "#file = \"plain_text.txt\" #name of the plain-text file\n",
    "#parser = PlaintextParser.from_file(file, Tokenizer(\"english\"))\n",
    "\n",
    "parser = PlaintextParser(text, Tokenizer(\"english\"))\n",
    "summarizer = LexRankSummarizer()\n",
    "\n",
    "summary = summarizer(parser.document, 5) #Summarize the document with 5 sentences\n",
    "\n",
    "for sentence in summary:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Rank using summa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/summanlp/textrank\n",
    "text = \"Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document. As the problem of information overload has grown, and as the quantity of data has increased, so has interest in automatic summarization. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax. An example of the use of summarization technology is search engines such as Google. Document summarization is another.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\n"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "print(summarizer.summarize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summarization\n",
      "text document\n",
      "original\n"
     ]
    }
   ],
   "source": [
    "#extract keywords\n",
    "from summa import keywords\n",
    "print(keywords.keywords(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\n",
      "An example of the use of summarization technology is search engines such as Google.\n",
      "Document summarization is another.\n"
     ]
    }
   ],
   "source": [
    "from summa.summarizer import summarize\n",
    "print(summarize(text, ratio=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\n",
      "\n",
      "Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\n",
      "Document summarization is another.\n",
      "\n",
      "Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\n",
      "As the problem of information overload has grown, and as the quantity of data has increased, so has interest in automatic summarization.\n",
      "An example of the use of summarization technology is search engines such as Google.\n",
      "Document summarization is another.\n",
      "\n",
      "Automatic summarization is the process of reducing a text document with a computer program in order to create a summary that retains the most important points of the original document.\n",
      "As the problem of information overload has grown, and as the quantity of data has increased, so has interest in automatic summarization.\n",
      "Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.\n",
      "An example of the use of summarization technology is search engines such as Google.\n",
      "Document summarization is another.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, words=20))\n",
    "print()\n",
    "print(summarize(text, words=40))\n",
    "print()\n",
    "print(summarize(text, words=60))\n",
    "print()\n",
    "print(summarize(text, words=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here are some other summarizers:\n",
    "\n",
    " - https://github.com/thavelick/summarize/ - Python, TF (very simple)\n",
    " - Reduction - Python, TextRank (simple)\n",
    " - Open Text Summarizer - C, TF without normalization\n",
    " - Simple program that summarize text - Python, TF without normalization\n",
    " - Intro to Computational Linguistics - Java, LexRank\n",
    " - Sumtract: Second project for UW LING 572 - Python\n",
    " - TextTeaser - Scala\n",
    " - PyTeaser - TextTeaser port in Python\n",
    " - Automatic Document Summarizer - Java, Bipartite HITS (no sources)\n",
    " - Pythia - Python, LexRank & Centroid\n",
    " - SWING - Ruby\n",
    " - Topic Networks - R, topic models & bipartite graphs\n",
    " - Almus: Automatic Text Summarizer - Java, LSA (without source code)\n",
    " - Musutelsa - Java, LSA (always freezes)\n",
    " - http://mff.bajecni.cz/index.php - C++\n",
    " - MEAD - Perl, various methods + evaluation framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
